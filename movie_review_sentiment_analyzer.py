# -*- coding: utf-8 -*-
"""Movie review sentiment analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x8qaBjsn18FAHd-XCwGn-9XFxv0ylCir

***Setup***
"""

from google.colab import drive
drive.mount('/content/drive')

"""***Get reviews from the web***"""

import requests   # library used to fetch web pages from the internet
from bs4 import BeautifulSoup   # library used to extract data from webpages

def get_imdb_reviews(url, max_reviews = 100):
  headers = {'User-Agent': 'Mozilla/5.0 (compatible)'}  # This helps the request appear as if it's coming from a web
  # browser
  r = requests.get(url, headers=headers, timeout = 10)  # send a GET request to the specified URL
  soup = BeautifulSoup(r.text, 'html.parser') # uses BeautifulSoup to parse the HTML content of the response (r.text)
  blocks = soup.find_all('div', class_='ipc-html-content-inner-div')  #  finds all div HTML elements that have the class
  # ipc-html-content-inner-div
  reviews = [b.get_text(strip = True) for b in blocks]  # extracts the text content from each of the found div elements
  return reviews[:max_reviews]  # returns a slice of the reviews list

"""***Fetch reviews from URL***"""

url = "https://www.imdb.com/title/tt11032374/reviews/?ref_=tt_ururv_sm" # URL for the reviews section of a specific
# movie on IMDb
reviews = get_imdb_reviews(url, max_reviews=50) # calls the get_imdb_reviews function
reviews = [clean_text(r) for r in reviews]  # This step is for cleaning and normalizing the text data
print("Number of reviews: ", len(reviews))  # prints the number of reviews that were successfully fetched and processed

"""***Preprocess reviews (cleaning and normalization)***"""

import re # imports the re module, which provides regular expression operations
# This module is commonly used for text pattern matching and manipulation

def clean_text(s):
  s = re.sub(r'\s', ' ', s) # This line replaces all whitespace characters (including tabs etc.) with a single space
  s = re.sub(r'http\S+', '', s) # removes any text that looks like a URL
  s = s.strip() # removes any leading or trailing whitespace from the string
  return s  # returns the cleaned string
reviews = [clean_text(r) for r in reviews]  # applies this clean_text function to each review in the reviews list

"""***Train simple model (Embedding + LSTM)***"""

import tensorflow_datasets as tfds

(train_ds, test_ds), info = tfds.load('imdb_reviews',
          split = ['train','test'],
          as_supervised = True, with_info = True)
# uses the tfds.load function to download and load the imdb_reviews dataset
# 'imdb_reviews': Specifies the name of the dataset to load.
# split = ['train','test']: Splits the dataset into training and testing sets.
# as_supervised = True: Loads the dataset as tuples of (features, labels), which is a common format for supervised learning.
# with_info = True: Returns information about the dataset in the info variable.

import tensorflow as tf
from tensorflow.keras import layers

vocab_size = 10000  # sets the maximum size of the vocabulary to 10,000.
# This means the TextVectorization layer will consider the 10,000 most frequent words in the dataset
max_len = 200 # sets the maximum length of the input sequences (reviews) to 200

vectorizer = layers.TextVectorization(max_tokens = vocab_size,  # Limits the vocabulary size to the value
                                                                # specified by vocab_size (10000)
                                      output_sequence_length = max_len) # Sets the output sequence length to the value
                                                                        # specified by max_len (200)
vectorizer.adapt(train_ds.map(lambda x,y: x)) # adapts the TextVectorization layer to the training dataset (train_ds)
# The .map(lambda x,y: x) part extracts only the text data (features) from the dataset
# The adapt method analyzes the text data to create a vocabulary of the most frequent words and
# prepares the layer for converting text to numerical sequences

def prepare(ds):  # defines a helper function called prepare which takes a TensorFlow dataset as input and
# prepares it for training or evaluation with the neural network model
  return ds.map(lambda x,y: (vectorizer(x), y)).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)
# return ds.map(lambda x,y: (vectorizer(x), y)): This part maps a function over the dataset ds
# .shuffle(10000): This shuffles the dataset with a buffer size of 10000
# .batch(32): This batches the data into groups of 32 samples
# .prefetch(tf.data.AUTOTUNE): This prefetches data to overlap the data preprocessing and model execution

train_data = prepare(train_ds)  # calls the prepare function with the train_ds dataset
test_data = prepare(test_ds)  # calls the prepare function with the test_ds dataset

"""***Build Model***"""

from tensorflow.keras import models, layers

model = models.Sequential([layers.Embedding(vocab_size, 64, input_length = max_len),
                          #  This layer learns a representation for each word in the vocabulary
                           layers.Bidirectional(layers.LSTM(64)),
                          #  input sequence is processed in both forward and backward directions
                           layers.Dense(64, activation = 'relu'),
                          #  Dense layers are used to learn complex patterns in the data
                           layers.Dropout(0.5), # helps prevent overfitting
                           layers.Dense(1, activation = 'sigmoid')])  # probability of the review being positive
model.compile('adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # compiles the model
# 'adam': Specifies the optimizer to use, which is Adam in this case
# Optimizers are algorithms that adjust the model's weights during training to minimize the loss
# loss = 'binary_crossentropy': Specifies the loss function to use and is a common loss function for binary classification
# metrics = ['accuracy']: Specifies the metrics to monitor during training which is accuracy in this case
model.fit(train_data, validation_data = test_data, epochs = 5)  # trains the model

"""***Predict sentiment for many reviews***"""

import numpy as np

texts = reviews # assigns the reviews list (cleaned review text) to the variable texts
vecs = vectorizer(texts)  # applies the vectorizer to the texts
if len(vecs) > 0: # If the reviews list was empty, vecs would also be empty
  preds = model.predict(vecs) # uses the trained model to predict the sentiment for each vectorized review
else:
  print("No reviews to predict on.")  # prints a message indicating that there are no reviews to predict on

"""***Aggregate results into Mostly Positive / Mostly Negative / Mixed***"""

positive_count = sum(p>0.5 for p in preds)  # counts how many of the predicted probabilities are greater than 0.5
negative_count = len(preds) - positive_count  # calculates the number of negative predictions
total = len(preds)  # stores the total number of predictions in the total variable

if positive_count / total > 0.6:  # checks if the proportion of positive predictions is greater than 60%
  overall = "Mostly Positive"
elif negative_count / total > 0.6:  # checks if the proportion of negative predictions is greater than 60%
  overall = "Mostly Negative"
else:
  overall = "Mixed"
  # If neither the positive nor negative predictions exceed 60%, the overall sentiment is set to "Mixed"
print(overall)

"""Visualize and Inspect"""

import matplotlib.pyplot as plt

plt.clf()   # clear the current figure
plt.close() # close any previous plots
plt.hist(preds, bins = 20)  # creates the histogram
plt.title("Sentiment probability distribution") # sets the title of the histogram
plt.xlabel("Probability (positive)")  # sets the label for the x-axis
plt.show()  # displays the generated histogram

"""The histogram visualizes the distribution of the predicted sentiment probabilities for the reviews.

*   **X-axis (Probability (positive))**: Represents the predicted probability that a review is positive. A value close to 0 indicates a high probability of the review being negative, while a value close to 1 indicates a high probability of the review being positive. Values in between suggest a mixed or neutral sentiment.
*   **Y-axis**: Represents the count of reviews that fall within each probability range (bin) on the x-axis. In other words, it shows how many reviews were predicted to have a sentiment probability within a specific interval.

Looking at the histogram, you can see the frequency of reviews predicted as mostly negative (closer to 0), mostly positive (closer to 1), and those with less clear sentiment. In this case, there's a significant number of reviews predicted as highly negative and a smaller number predicted as highly positive, which aligns with the overall sentiment being classified as "Mixed".
"""

!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook --inplace Movie_review_sentiment_analyzer.ipynb